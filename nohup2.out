['/home/ubuntu/ardino/mjjeon/nsynth-pytorch', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/ubuntu/.local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/home/ubuntu/ardino/mjjeon/nsynth-pytorch', '/home/ubuntu/ardino/mjjeon/nsynth-pytorch', '/home/ubuntu/ardino/mjjeon/nsynth-pytorch', '/home/ubuntu/ardino/mjjeon/nsynth-pytorch']
make model is done.
Loading NSynth data from split train at /home/ubuntu/ardino/mjjeon/nsynth-pytorch/data/nsynth-train
	Found 9 samples.
Loading NSynth data from split test at /home/ubuntu/ardino/mjjeon/nsynth-pytorch/data/nsynth-test
	Found 2 samples.
make loaders is done.
train start!

Using device: cuda:0
train until  250000
it=         0		Loss/train:2.948e+03	Time/train:6.013e+00	LR:1.000e-03
SAVE THE MODEL
TEST THE MODEL
/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
it=         0		Loss/test:4.879e+01	Time/test:3.072e+00
/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
it=        20		Loss/train:6.788e+01	Time/train:5.146e+00	LR:1.000e-03
it=        40		Loss/train:1.047e+01	Time/train:5.171e+00	LR:1.000e-03
it=        60		Loss/train:8.911e+00	Time/train:5.166e+00	LR:1.000e-03
it=        80		Loss/train:9.088e+00	Time/train:5.172e+00	LR:1.000e-03
it=       100		Loss/train:9.458e+00	Time/train:5.115e+00	LR:1.000e-03
it=       120		Loss/train:8.918e+00	Time/train:5.147e+00	LR:1.000e-03
it=       140		Loss/train:8.055e+00	Time/train:5.161e+00	LR:1.000e-03
it=       160		Loss/train:8.832e+00	Time/train:5.144e+00	LR:1.000e-03
it=       180		Loss/train:8.203e+00	Time/train:5.086e+00	LR:1.000e-03
it=       200		Loss/train:6.091e+00	Time/train:5.095e+00	LR:1.000e-03
it=       220		Loss/train:7.770e+00	Time/train:5.111e+00	LR:1.000e-03
it=       240		Loss/train:7.347e+00	Time/train:5.106e+00	LR:1.000e-03
it=       260		Loss/train:6.868e+00	Time/train:5.125e+00	LR:1.000e-03
it=       280		Loss/train:7.680e+00	Time/train:5.100e+00	LR:1.000e-03
it=       300		Loss/train:8.611e+00	Time/train:5.116e+00	LR:1.000e-03
it=       320		Loss/train:6.747e+00	Time/train:5.061e+00	LR:1.000e-03
it=       340		Loss/train:7.511e+00	Time/train:5.160e+00	LR:1.000e-03
it=       360		Loss/train:7.315e+00	Time/train:5.252e+00	LR:1.000e-03
it=       380		Loss/train:7.549e+00	Time/train:5.287e+00	LR:1.000e-03
it=       400		Loss/train:7.664e+00	Time/train:5.251e+00	LR:1.000e-03
it=       420		Loss/train:7.649e+00	Time/train:5.133e+00	LR:1.000e-03
it=       440		Loss/train:8.370e+00	Time/train:5.149e+00	LR:1.000e-03
it=       460		Loss/train:6.381e+00	Time/train:5.092e+00	LR:1.000e-03
it=       480		Loss/train:8.931e+00	Time/train:5.137e+00	LR:1.000e-03
it=       500		Loss/train:6.241e+00	Time/train:5.090e+00	LR:1.000e-03
TEST THE MODEL
it=       500		Loss/test:5.224e+00	Time/test:2.979e+00
/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
it=       520		Loss/train:7.563e+00	Time/train:5.120e+00	LR:1.000e-03
it=       540		Loss/train:8.299e+00	Time/train:5.118e+00	LR:1.000e-03
it=       560		Loss/train:9.267e+00	Time/train:5.138e+00	LR:1.000e-03
it=       580		Loss/train:8.939e+00	Time/train:5.127e+00	LR:1.000e-03
it=       600		Loss/train:5.896e+00	Time/train:5.110e+00	LR:1.000e-03
it=       620		Loss/train:7.524e+00	Time/train:5.124e+00	LR:1.000e-03
it=       640		Loss/train:6.384e+00	Time/train:5.080e+00	LR:1.000e-03
it=       660		Loss/train:7.168e+00	Time/train:5.150e+00	LR:1.000e-03
it=       680		Loss/train:7.711e+00	Time/train:5.137e+00	LR:1.000e-03
it=       700		Loss/train:6.213e+00	Time/train:5.148e+00	LR:1.000e-03
it=       720		Loss/train:5.780e+00	Time/train:5.168e+00	LR:1.000e-03
it=       740		Loss/train:6.268e+00	Time/train:5.144e+00	LR:1.000e-03
it=       760		Loss/train:6.463e+00	Time/train:5.133e+00	LR:1.000e-03
it=       780		Loss/train:9.361e+00	Time/train:5.096e+00	LR:1.000e-03
it=       800		Loss/train:7.083e+00	Time/train:5.136e+00	LR:1.000e-03
it=       820		Loss/train:8.291e+00	Time/train:5.130e+00	LR:1.000e-03
it=       840		Loss/train:6.956e+00	Time/train:5.135e+00	LR:1.000e-03
it=       860		Loss/train:7.508e+00	Time/train:5.096e+00	LR:1.000e-03
it=       880		Loss/train:7.040e+00	Time/train:5.114e+00	LR:1.000e-03
it=       900		Loss/train:5.675e+00	Time/train:5.159e+00	LR:1.000e-03
it=       920		Loss/train:7.280e+00	Time/train:5.260e+00	LR:1.000e-03
it=       940		Loss/train:6.808e+00	Time/train:5.231e+00	LR:1.000e-03
it=       960		Loss/train:6.417e+00	Time/train:5.263e+00	LR:1.000e-03
it=       980		Loss/train:6.183e+00	Time/train:5.270e+00	LR:1.000e-03
it=      1000		Loss/train:6.715e+00	Time/train:5.186e+00	LR:1.000e-03
TEST THE MODEL
it=      1000		Loss/test:4.044e+00	Time/test:2.949e+00
Traceback (most recent call last):
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/train.py", line 34, in <module>
    main(make_config('train').parse_args())
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/train.py", line 19, in main
    train(model=model,
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/nsynth/training.py", line 88, in train
    _, loss = loss_function(model, x, y, device)
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/nsynth/vae.py", line 63, in loss_function
    logits, x_q, x_q_log_prob = model(x)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 183, in forward
    return self.module(*inputs[0], **module_kwargs[0])
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/nsynth/vae.py", line 57, in forward
    logits = self.decoder(x, x_q)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ardino/mjjeon/nsynth-pytorch/nsynth/decoder.py", line 149, in forward
    dilated = dilated + self.upsampler(cond(embedding))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacty of 31.75 GiB of which 143.75 MiB is free. Including non-PyTorch memory, this process has 31.61 GiB memory in use. Of the allocated memory 24.82 GiB is allocated by PyTorch, and 5.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
